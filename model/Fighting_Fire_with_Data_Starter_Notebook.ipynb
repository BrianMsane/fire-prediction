{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install yellow-brick keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "2lLzfdilxfsY"
      },
      "outputs": [],
      "source": [
        "# import keras_tuner as kt \n",
        "# from yellowbrick.model_selection import RFECV\n",
        "\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import RidgeCV\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 342
        },
        "id": "uaQM7lJHxRoe",
        "outputId": "c1992756-e707-443b-b85b-0289d670ec34"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((83148, 29), (25584, 28))"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The training data.\n",
        "train = pd.read_csv('../data/Train.csv')\n",
        "test = pd.read_csv('../data/Test.csv')\n",
        "ss = pd.read_csv('../data/SampleSubmission.csv')\n",
        "train.shape, test.shape\n",
        "\n",
        "\n",
        "# // The training data has many features so that could lead to a curse of dimensionality \n",
        "    # // dimentionality reduction - PCA and other methods\n",
        "    # // feature selection for importance features ()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# // only the target is not present in the testing set\n",
        "\n",
        "for column in train.columns:\n",
        "    if column not in test.columns:\n",
        "        print(column)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# // Investigate missing values\n",
        "    # // There are no missing values\n",
        "\n",
        "train.isnull().any().any(), test.isnull().any().any(), ss.isnull().any().any()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "1WCG4h0-S_v1",
        "outputId": "ce07db1b-78b5-48f7-fe5f-56a74699589d"
      },
      "outputs": [],
      "source": [
        "# Look at distribution of each variable\n",
        "train.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# // The target is highly skewed\n",
        "    # // we need to transform it\n",
        "\n",
        "print(train.burn_area.skew())\n",
        "\n",
        "sns.set_style('darkgrid')\n",
        "plt.figure(figsize = (13, 7))\n",
        "sns.histplot(train.burn_area)\n",
        "plt.title('household_size variable distribution', y = 1.02, fontsize = 15)\n",
        "display(plt.show())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "eF2NXmXbSOaW",
        "outputId": "6ac88e20-9ef9-4602-84ab-0f2bae1d276b"
      },
      "outputs": [],
      "source": [
        "# Look at correlation with target\n",
        "train.select_dtypes(include=['number']).corr()['burn_area'].sort_values().plot(kind='bar', figsize=(18, 6))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXIk62X9iONj"
      },
      "source": [
        "##  Adding date features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "pn-SKQxiHAon"
      },
      "outputs": [],
      "source": [
        "# Split the ID (eg 127_2017-01-03) to get the date string, which we convert to datetime to make life easier\n",
        "train['date'] = pd.to_datetime(train['ID'].apply(lambda x: x.split('_')[1]))\n",
        "test['date'] = pd.to_datetime(test['ID'].apply(lambda x: x.split('_')[1]))\n",
        "train['burn_area'] = pd.to_numeric(train['burn_area'], errors='coerce')\n",
        "\n",
        "\n",
        "# Date variables\n",
        "train['month'] = train.date.dt.month\n",
        "train['year'] = train.date.dt.year\n",
        "train['day'] = train.date.dt.weekday"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [],
      "source": [
        "# // Discretization\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "def optimal_k(data: pd.DataFrame, column: str, max_k: int=10):\n",
        "    silhouette_scores = []\n",
        "    X = data[column].values.reshape(-1, 1)\n",
        "    for k in range(2, max_k+1):\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        kmeans.fit(X)\n",
        "        score = silhouette_score(X, kmeans.labels_)\n",
        "        silhouette_scores.append(score)\n",
        "        optimal_k = range(2, max_k+1)[silhouette_scores.index(max(silhouette_scores))]\n",
        "    return optimal_k\n",
        "\n",
        "\n",
        "def binning(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    numerics = []\n",
        "    for i in data.columns:\n",
        "        if data[i].dtype == 'float' or data[i].dtype == 'int':\n",
        "            numerics.append(i)\n",
        "\n",
        "    for column in numerics:\n",
        "        k = optimal_k(data, column)\n",
        "        reshaped_data = data[column].values.reshape(-1, 1)\n",
        "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "        data[column] = kmeans.fit_predict(reshaped_data)\n",
        "    return data\n",
        "\n",
        "\n",
        "train = binning(train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# // Handling outliers\n",
        "    # // find other methods as well\n",
        "\n",
        "\n",
        "def capping(data: pd.DataFrame) -> pd.DataFrame:\n",
        "    numerics = []\n",
        "    for col in data.columns:\n",
        "        if data[col].dtype == 'float' or data[col].dtype == 'int':\n",
        "            numerics.append(col)\n",
        "\n",
        "    for col in numerics:\n",
        "        q1 = data[col].quantile(0.25)\n",
        "        q2 = data[col].quantile(0.75)\n",
        "        iqr = q2 - q1\n",
        "        max_limit = q2 + (1.5 * iqr)\n",
        "        min_limit = q1 - (1.5 * iqr)\n",
        "        data[col]  = pd.DataFrame(\n",
        "            np.where(data[col] > max_limit, max_limit,\n",
        "            (np.where(data[col] < min_limit, min_limit, data[col]))), columns=[col]\n",
        "        )\n",
        "    return data\n",
        "train = capping(train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# // Splitting the data into training and testing sets\n",
        "\n",
        "train_all = train.copy().dropna()\n",
        "train = train_all.loc[train_all.date < '2011-01-01']\n",
        "valid = train_all.loc[train_all.date > '2011-01-01']\n",
        "print(train.shape, valid.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "hMjgH87RzOqX"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "\"['dateclimate_pdsi'] not in index\"",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[61], line 12\u001b[0m\n\u001b[0;32m      2\u001b[0m in_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimate_aet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimate_def\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimate_pdsi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimate_pet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimate_pr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimate_ro\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimate_soil\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimate_srad\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimate_swe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclimate_tmmn\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlandcover_4\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlandcover_5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlandcover_6\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlandcover_7\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlandcover_8\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecipitation\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      9\u001b[0m target_col \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mburn_area\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 12\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m[\u001b[49m\u001b[43min_cols\u001b[49m\u001b[43m]\u001b[49m, train[target_col]\n\u001b[0;32m     13\u001b[0m X_valid, y_valid \u001b[38;5;241m=\u001b[39m valid[in_cols], valid[target_col]\n",
            "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
            "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mKeyError\u001b[0m: \"['dateclimate_pdsi'] not in index\""
          ]
        }
      ],
      "source": [
        "# Define input and output columns - you can play with adding or removing inputs to the model\n",
        "in_cols = ['climate_aet', 'climate_def', 'month', 'year', 'day', 'date'\n",
        "       'climate_pdsi', 'climate_pet', 'climate_pr', 'climate_ro',\n",
        "       'climate_soil', 'climate_srad', 'climate_swe', 'climate_tmmn',\n",
        "       'climate_tmmx', 'climate_vap', 'climate_vpd', 'climate_vs', 'elevation',\n",
        "       'landcover_0', 'landcover_1', 'landcover_2', 'landcover_3',\n",
        "       'landcover_4', 'landcover_5', 'landcover_6', 'landcover_7',\n",
        "       'landcover_8', 'precipitation']\n",
        "target_col = 'burn_area'\n",
        "\n",
        "\n",
        "X_train, y_train = train[in_cols], train[target_col]\n",
        "X_valid, y_valid = valid[in_cols], valid[target_col]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# // Feature selection\n",
        "\n",
        "visualizer = RFECV(\n",
        "    RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1),\n",
        "    step=1, cv=5, scoring=\"mean_squared_error\"\n",
        ")\n",
        "visualizer.fit(X_train, y_train)\n",
        "plt.figure(figsize=[5.6, 4])\n",
        "visualizer.show()\n",
        "\n",
        "\n",
        "def feature_selection(X_train, X_test, y_train, index: int=16) -> tuple:\n",
        "    sel = RFE(\n",
        "        estimator=RandomForestRegressor(\n",
        "            n_estimators=100, random_state=42, n_jobs=-1\n",
        "        ),\n",
        "        n_features_to_select=index\n",
        "    )\n",
        "    sel.fit(X_train, y_train)\n",
        "    X_train_rfe = sel.transform(X_train) \n",
        "    X_test_rfe = sel.transform(X_test)\n",
        "    return X_train_rfe, X_test_rfe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "def scaler(X_train, X_test, minmax: bool=False):\n",
        "    stdscaler = StandardScaler()\n",
        "    X_train = stdscaler.fit_transform(X_train)\n",
        "    X_test = stdscaler.transform(X_test)\n",
        "\n",
        "    if minmax:\n",
        "        minmaxscaler = MinMaxScaler(feature_range=(0,1))\n",
        "        X_train = minmaxscaler.fit_transform(X_train)\n",
        "        X_test = minmaxscaler.transform(X_test)\n",
        "    return X_train, X_test\n",
        "\n",
        "X_train, X_valid = scaler(X_train=X_train, X_test=X_valid, minmax=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# // PCA\n",
        "\n",
        "\n",
        "pca = PCA(n_components=1, random_state=42)\n",
        "pca = pca.fit(X_train)\n",
        "X_train = pca.transform(X_train)\n",
        "X_valid = pca.transform(X_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-8j9zDpzUpc",
        "outputId": "dbd40957-1257-4126-9200-1105a10993ad"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create and fit the model\n",
        "model = RidgeCV()\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_valid)\n",
        "mean_squared_error(y_valid, preds)**0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Create and fit the model\n",
        "random_F = RandomForestRegressor(random_state=42)\n",
        "random_F.fit(X_train, y_train)\n",
        "preds = random_F.predict(X_valid)\n",
        "mean_squared_error(y_valid, preds)**0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.07291198056640737"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Create and fit the model\n",
        "svr = SVR(kernel='poly', gamma='auto', C=10)\n",
        "svr.fit(X_train, y_train)\n",
        "preds = svr.predict(X_valid)\n",
        "mean_squared_error(y_valid, preds)**0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.024208812663517574"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "\n",
        "ada = AdaBoostRegressor(random_state=42, estimator=random_F, learning_rate=1e-4)\n",
        "ada.fit(X_train, y_train)\n",
        "preds = ada.predict(X_valid)\n",
        "mean_squared_error(y_valid, preds)**0.5 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.026961837079682565"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "kn = KNeighborsRegressor(n_neighbors=3)\n",
        "kn.fit(X_train, y_train)\n",
        "preds = kn.predict(X_valid)\n",
        "mean_squared_error(y_valid, preds)**0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# // Cross-validation\n",
        "\n",
        "\n",
        "X = train[in_cols]\n",
        "y = train['burn_area']\n",
        "cv = KFold(n_splits=10, shuffle=False)\n",
        "\n",
        "scores = []\n",
        "for train_index, test_index in cv.split(X=X, y=y):\n",
        "        X_train = X[train_index]\n",
        "        y_train = y[train_index]\n",
        "        X_test = X[test_index]\n",
        "        y_test = y[test_index]\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_test)\n",
        "        mean_squared_error(y_test, preds)**0.5\n",
        "        scores.append(mean_squared_error(y_test, preds)**0.5)\n",
        "\n",
        "print(\"The average score is {}\".format(np.mean(scores)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "isI_-sfnzXED"
      },
      "outputs": [],
      "source": [
        "# // more models\n",
        "\n",
        "\n",
        "def grid_search(model, params):\n",
        "    grid = GridSearchCV(model, params, cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "    grid.fit(X_train, y_train)\n",
        "    return grid.best_estimator_, grid.best_params_, np.sqrt(-grid.best_score_)\n",
        "\n",
        "\n",
        "models = {\n",
        "    \"Linear Regression\": {\n",
        "        \"model\": LinearRegression(),\n",
        "        \"params\": {}\n",
        "    },\n",
        "    \"Ridge Regression\": {\n",
        "        \"model\": Ridge(random_state=42, alpha=1, solver='svd'),\n",
        "        \"params\": {\"alpha\": [0.1, 1, 10, 100]}\n",
        "    },\n",
        "    \"Random Forest Regression\": {\n",
        "        \"model\": RandomForestRegressor(random_state=42),\n",
        "        \"params\": {\"n_estimators\": [10, 50, 100], \"max_depth\": [None, 10, 20, 30]}\n",
        "    },\n",
        "    \"Gradient Boosting Regression\": {\n",
        "        \"model\": GradientBoostingRegressor(random_state=42),\n",
        "        \"params\": {\"n_estimators\": [100, 200], \"learning_rate\": [0.01, 0.1], \"max_depth\": [3, 5, 7]}\n",
        "    },\n",
        "    \"K-Nearest Neighbors Regression\": {\n",
        "        \"model\": KNeighborsRegressor(),\n",
        "        \"params\": {\"n_neighbors\": [3, 5, 7], \"weights\": ['uniform', 'distance']}\n",
        "    }\n",
        "}\n",
        "\n",
        "# models to add\n",
        "    # // LightGBM\n",
        "    # // "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "xmK8kfNU7agI"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Regression - Best Params: {}, RMSE: 0.02733069747339375\n",
            "Ridge Regression - Best Params: {'alpha': 100}, RMSE: 0.027330800334030417\n",
            "Random Forest Regression - Best Params: {'max_depth': 10, 'n_estimators': 100}, RMSE: 0.026179369068177295\n",
            "Gradient Boosting Regression - Best Params: {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 200}, RMSE: 0.025617402169930487\n",
            "K-Nearest Neighbors Regression - Best Params: {'n_neighbors': 7, 'weights': 'distance'}, RMSE: 0.025250276182806154\n",
            "\n",
            "Best Model: K-Nearest Neighbors Regression\n",
            "Test RMSE: 0.02514277990263765\n"
          ]
        }
      ],
      "source": [
        "# // Hyper-parameter tuning\n",
        "\n",
        "\n",
        "results = {}\n",
        "for name, config in models.items():\n",
        "    model, params, score = grid_search(config['model'], config['params'])\n",
        "    results[name] = {'Best Model': model, 'Best Params': params, 'RMSE': score}\n",
        "\n",
        "# Print the results\n",
        "for name, result in results.items():\n",
        "    print(f\"{name} - Best Params: {result['Best Params']}, RMSE: {result['RMSE']}\")\n",
        "\n",
        "\n",
        "best_model_name = min(results, key=lambda x: results[x]['RMSE'])\n",
        "best_model = results[best_model_name]['Best Model']\n",
        "test_rmse = np.sqrt(mean_squared_error(y_valid, best_model.predict(X_valid)))\n",
        "\n",
        "print(f\"\\nBest Model: {best_model_name}\")\n",
        "print(f\"Test RMSE: {test_rmse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # // Denoising Auto-Encoder\n",
        "\n",
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.layers import Input, Dense\n",
        "# from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "# # Add noise to the data\n",
        "# # noise_factor = 0.5\n",
        "# # x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "# # x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "# # x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "# # x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
        "\n",
        "\n",
        "# input_img = Input(shape=(784,))\n",
        "# encoded = Dense(128, activation='relu')(input_img)\n",
        "# encoded = Dense(64, activation='relu')(encoded)\n",
        "# encoded = Dense(32, activation='relu')(encoded) # bottleneck layer\n",
        "\n",
        "# decoded = Dense(64, activation='relu')(encoded)\n",
        "# decoded = Dense(128, activation='relu')(decoded)\n",
        "# decoded = Dense(784, activation='sigmoid')(decoded)\n",
        "\n",
        "# autoencoder = Model(input_img, decoded)\n",
        "# autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "\n",
        "# # Train the autoencoder\n",
        "# autoencoder.fit(x_train_noisy, x_train,\n",
        "#                 epochs=50,\n",
        "#                 batch_size=256,\n",
        "#                 shuffle=True,\n",
        "#                 validation_data=(x_test_noisy, x_test))\n",
        "\n",
        "# # Use the encoder part of the autoencoder to get the encoded representations\n",
        "# encoder = Model(input_img, encoded)\n",
        "# encoded_imgs = encoder.predict(x_train_noisy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# // Deep Learning Model\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units=512, activation='relu'))\n",
        "model.add(Dropout(rate=.2))\n",
        "model.add(Dense(units=64, activation='sigmoid'))\n",
        "model.add(Dense(units=32, activation='sigmoid'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "            loss='sparse_categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "model.fit(X_train, y_train)\n",
        "preds = model.predict(X_valid)\n",
        "mean_squared_error(y_valid, preds)**0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# // Hyper-parameter tuning - Keras-tuner\n",
        "\n",
        "\n",
        "\n",
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(units=hp.Int('units', min_value=32, max_value=512, step=32), activation='relu', input_shape=(32,)))\n",
        "    model.add(Dropout(rate=hp.Float('dropout', min_value=0.0, max_value=0.5, step=0.1)))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    \n",
        "    model.compile(optimizer=hp.Choice('optimizer', values=['adam', 'rmsprop']),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "\n",
        "tuner = kt.Hyperband(\n",
        "    build_model,\n",
        "    objective='val_accuracy',\n",
        "    max_epochs=50,\n",
        "    factor=3,\n",
        "    directory='my_dir',\n",
        "    project_name='intro_to_kt'\n",
        ")\n",
        "\n",
        "\n",
        "tuner.search(X_train, y_train, epochs=50)#, validation_data=(X_valid, y_valid))\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "model = tuner.hypermodel.build(best_hps)\n",
        "history = model.fit(X_train, y_train, epochs=50)#, validation_data=(X_valid, y_valid))\n",
        "\n",
        "# Evaluate the model\n",
        "eval_result = model.evaluate(X_valid, y_valid)\n",
        "print(f\"Test accuracy: {eval_result[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq7DM0URkkai"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "HItSe6MCkyPB",
        "outputId": "02b1ac1b-05bd-43c3-f3cf-13557c4a42f5"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "test = test.drop(['ID'], axis=1)\n",
        "test['month'] = test.date.dt.month\n",
        "test['year'] = test.date.dt.year\n",
        "test['data'] = test.date.dt.dayofweek\n",
        "\n",
        "\n",
        "test = pca.transform(test[in_cols])\n",
        "preds = model.predict(test)#[in_cols].fillna(0))\n",
        "ss['burn_area'] = preds\n",
        "\n",
        "ss['burn_area'] = ss['burn_area'].clip(0, 1)\n",
        "ss.to_csv('starter_submission.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
