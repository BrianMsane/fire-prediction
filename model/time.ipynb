{"cells":[{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":825,"status":"ok","timestamp":1723807246902,"user":{"displayName":"Leonard Mwangi","userId":"02363524813695713194"},"user_tz":-180},"id":"2lLzfdilxfsY"},"outputs":[],"source":["import pandas as pd\n","from pmdarima import auto_arima\n","from statsmodels.tsa.statespace.sarimax import SARIMAX\n","from pykalman import KalmanFilter\n","from sklearn.cluster import KMeans\n","import warnings\n","warnings.filterwarnings('ignore')"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":2954,"status":"ok","timestamp":1723787465678,"user":{"displayName":"Leonard Mwangi","userId":"02363524813695713194"},"user_tz":-180},"id":"WAPM7kiAxVFG"},"outputs":[],"source":["train = pd.read_csv('../data/Train.csv')\n","test = pd.read_csv('../data/Test.csv')\n","sub = pd.read_csv('../data/SampleSubmission.csv')"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# //  Date variables\n","\n","# train['burn_area'] = train.burn_area ** float(1/1024)\n","train['area'] = pd.to_numeric(train['ID'].apply(lambda x: x.split('_')[0]))\n","train['date'] = pd.to_datetime(train['ID'].apply(lambda x: x.split('_')[1]))\n","train['burn_area'] = pd.to_numeric(train['burn_area'], errors='coerce')\n","\n","train['month'] = train.date.dt.month\n","train['year'] = train.date.dt.year\n","train['day'] = train.date.dt.weekday\n","train['quarter'] = train.date.dt.quarter\n","train['dayofmonth'] = train.date.dt.day\n","train['dayofyear'] = train.date.dt.dayofyear\n","train['weekofyear'] = train.date.dt.isocalendar().week\n","# train.set_index('date', inplace=True)\n","\n","# // feature engineering features\n","\n","\n","# train['interaction_vpd_landcover1'] = train['climate_vpd'] * train['landcover_1']\n","# train['interaction_tmmx_vs'] = train['climate_tmmx'] * train['climate_vs']\n","# train['lag_climate_pr'] = train.groupby('area')['climate_pr'].shift(1)\n","# train['lag_climate_vpd'] = train.groupby('area')['climate_vpd'].shift(1)\n","# spatial_features = train[['lat', 'lon', 'elevation']].dropna()\n","# kmeans = KMeans(n_clusters=5, random_state=42).fit(spatial_features)\n","# train['spatial_cluster'] = kmeans.predict(train[['lat', 'lon', 'elevation']])\n","# train['burn_risk_index'] = (train['climate_vpd'] + train['climate_pdsi'] + train['climate_tmmx'] - train['climate_pr'])\n","\n","\n","# // Add lag featues to make it time series\n","\n"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# //  Date variables\n","\n","# train['burn_area'] = train.burn_area ** float(1/2)\n","test['area'] = pd.to_numeric(test['ID'].apply(lambda x: x.split('_')[0]))\n","test['date'] = pd.to_datetime(test['ID'].apply(lambda x: x.split('_')[1]))\n","\n","test['month'] = test.date.dt.month\n","test['year'] = test.date.dt.year\n","test['day'] = test.date.dt.weekday\n","test['quarter'] = test.date.dt.quarter\n","test['dayofmonth'] = test.date.dt.day\n","test['dayofyear'] = test.date.dt.dayofyear\n","test['weekofyear'] = test.date.dt.isocalendar().week\n","# train.set_index('date', inplace=True)\n","\n","# // feature engineering features\n","\n","\n","# test['interaction_vpd_landcover1'] = test['climate_vpd'] * test['landcover_1']\n","# test['interaction_tmmx_vs'] = test['climate_tmmx'] * test['climate_vs']\n","# test['lag_climate_pr'] = test.groupby('area')['climate_pr'].shift(1)\n","# test['lag_climate_vpd'] = test.groupby('area')['climate_vpd'].shift(1)\n","# test['spatial_cluster'] = kmeans.predict(test[['lat', 'lon', 'elevation']])\n","# test['burn_risk_index'] = (test['climate_vpd'] + test['climate_pdsi'] + test['climate_tmmx'] - test['climate_pr'])\n","\n","\n","# // Add lag featues to make it time series\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":547,"status":"ok","timestamp":1723787481690,"user":{"displayName":"Leonard Mwangi","userId":"02363524813695713194"},"user_tz":-180},"id":"pn-SKQxiHAon"},"outputs":[],"source":["# \n","# train['date'] = pd.to_datetime(train['ID'].apply(lambda x: x.split('_')[1]))\n","# train['month'] = train.date.dt.month\n","# train['year'] = train.date.dt.year\n","# train['area'] = pd.to_numeric(train['ID'].apply(lambda x: x.split('_')[0]))\n","train.set_index('date', inplace=True)\n","\n","# # Apply same procedure on test set\n","# test['date'] = pd.to_datetime(test['ID'].apply(lambda x: x.split('_')[1]))\n","# test['area'] = pd.to_numeric(test['ID'].apply(lambda x: x.split('_')[0]))\n","test.set_index('date', inplace=True)\n","\n","# Logically splitting data\n","X_train = train.loc[train.index.strftime('%Y-%m-%d') < '2013-01-01']\n","y_test = train.loc[train.index.strftime('%Y-%m-%d') >= '2013-01-01']"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# from itertools import product\n","# import numpy as np\n","\n","# # Define the parameter grid\n","# p = d = q = range(0, 3)\n","# pdq = list(product(p, d, q))\n","# seasonal_pdq = [(x[0], x[1], x[2], 12) for x in pdq]\n","\n","# best_aic = np.inf\n","# best_params = None\n","\n","# for param in pdq:\n","#     for param_seasonal in seasonal_pdq:\n","#         try:\n","#             mod = SARIMAX(train['burn_area'],\n","#                           order=param,\n","#                           seasonal_order=param_seasonal,\n","#                           enforce_stationarity=False,\n","#                           enforce_invertibility=False)\n","#             results = mod.fit()\n","#             if results.aic < best_aic:\n","#                 best_aic = results.aic\n","#                 best_params = (param, param_seasonal)\n","#         except:\n","#             continue\n","\n","# print(f'Best SARIMA parameters: {best_params}')\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1474858,"status":"ok","timestamp":1723792925287,"user":{"displayName":"Leonard Mwangi","userId":"02363524813695713194"},"user_tz":-180},"id":"z27wDVK9FKR_","outputId":"0733e2d2-6d73-451f-ecc2-738370031fa2"},"outputs":[],"source":["\n","preds = list()\n","\n","for area in train['area'].unique():\n","    train_area = train.loc[train.area == area]\n","    test_area = test.loc[test.area == area]\n","    try:\n","        sarima_fit = SARIMAX(\n","            train_area['burn_area'],\n","            order=(2, 0, 0),\n","            seasonal_order=(2, 0, 1, 12)\n","        ).fit()\n","        forecast_months = len(test_area)\n","        sarimax_pred = sarima_fit.get_forecast(steps=forecast_months, freq='MS')\n","        predicted_values = sarimax_pred.predicted_mean\n","\n","        kf = KalmanFilter(initial_state_mean=predicted_values.iloc[0], n_dim_obs=1)\n","        predicted_values_kf, _ = kf.filter(predicted_values.values)\n","        predicted_data_kf = pd.Series(predicted_values_kf.flatten(), index=test_area.index, name='burn_area')\n","        predicted_data_df = pd.DataFrame(\n","            {\n","                'ID': str(area)+'_'+test_area.index.strftime('%Y-%m-%d').map(str),\n","                'burn_area': predicted_data_kf.values\n","            }\n","        )\n","\n","    except Exception as e:\n","        model_arima= auto_arima(\n","            train_area['burn_area'], \n","            m=12,\n","            seasonal=True,\n","            start_p=0,\n","            start_q=0,\n","            max_order=5,\n","            test='adf',\n","            error_action= 'ignore',\n","            suppress_warnings=True,\n","            stepwise= True, trace= True\n","        )\n","\n","        forecast_months = len(test_area)\n","        predicted_values = model_arima.predict(n_periods=forecast_months)\n","        kf = KalmanFilter(initial_state_mean=predicted_values.iloc[0], n_dim_obs=1)\n","        predicted_values_kf, _ = kf.filter(predicted_values.values)\n","        predicted_data_kf = pd.Series(predicted_values_kf.flatten(), index=test_area.index, name='burn_area')\n","\n","        predicted_data_df = pd.DataFrame(\n","            {\n","                'ID': str(area) + '_' + test_area.index.strftime('%Y-%m-%d').map(str),\n","                'burn_area': predicted_data_kf.values\n","            }\n","        )\n","    preds.append(predicted_data_df)"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["# Univariate SARIMAX with train-test split and Kalman Filter applied to the predictions\n","# and Auto ARIMA for areas where SARIMAX returns an error\n","import numpy as np\n","from sklearn.metrics import mean_squared_error\n","# Initialize an empty list to RMSE values for each area\n","rmse_values = []\n","predictions = []\n","\n","# Loop through each area ID\n","for real_id in train['area'].unique():\n","    # Filter the train and test data for the current area ID\n","    train_area = train.loc[train.area == real_id]\n","    test_area = test.loc[test.area == real_id]\n","\n","    # Train Test Split\n","    X_train_area = train_area.loc[train_area.index.strftime('%Y-%m-%d') < '2011-01-01']\n","    y_test_area = train_area.loc[train_area.index.strftime('%Y-%m-%d') >= '2011-01-01']\n","\n","    try:\n","        # Auto ARIMA model tuning\n","        model_arima = auto_arima(X_train_area['burn_area'], m=12, seasonal=True,\n","                                  start_p=0, start_q=0, max_order=5, test='adf', error_action='ignore',\n","                                  suppress_warnings=True, stepwise=True, trace=False)\n","        # Set best order as the values obtained via auto ARIMA\n","        non_seasonal_order = model_arima.order\n","        best_seasonal_order = model_arima.seasonal_order\n","\n","    except Exception as e:\n","        print(f\"Failed to tune real_id {real_id}: {e}. Falling back to default seasonal and non-seasonal order\")\n","        # Default best order terms\n","        non_seasonal_order = (2, 0, 0)\n","        best_seasonal_order = (2, 0, 1, 12)\n","\n","    try:\n","        # Fit SARIMA model with the best orders on train set\n","        sarimax_model = SARIMAX(X_train_area['burn_area'],\n","                                order=non_seasonal_order,\n","                                seasonal_order=best_seasonal_order)\n","        sarima_fit = sarimax_model.fit()\n","\n","        # Forecast for the specific number of months\n","        forecast_months = len(y_test_area)\n","\n","        # Perform the forecast\n","        sarimax_pred = sarima_fit.get_forecast(steps=forecast_months, freq='MS')\n","\n","        # Extract predicted values\n","        predicted_values = sarimax_pred.predicted_mean\n","\n","        # Apply Kalman Filter to smooth the predictions\n","        kf = KalmanFilter(initial_state_mean=predicted_values.iloc[0], n_dim_obs=1)\n","        predicted_values_kf, _ = kf.filter(predicted_values.values)\n","        predicted_data_df = pd.Series(predicted_values_kf.flatten(), index=y_test_area.index, name='burn_area')\n","\n","    except Exception as e:\n","        print(f\"Failed to model real_id {real_id}: {e}. Generating ARIMA forecast\")\n","        try:\n","            # Auto ARIMA model tuning\n","            model_arima = auto_arima(X_train_area['burn_area'], m=12, seasonal=True,\n","                                      start_p=0, start_q=0, max_order=5, test='adf', error_action='ignore',\n","                                      suppress_warnings=True, stepwise=True, trace=False)\n","            forecast_length = len(y_test_area)\n","            predicted_values = model_arima.predict(n_periods=forecast_length)\n","            # Apply Kalman Filter to smooth the predictions\n","            kf = KalmanFilter(initial_state_mean=predicted_values.iloc[0], n_dim_obs=1)\n","            predicted_values_kf, _ = kf.filter(predicted_values.values)\n","            predicted_data_df = pd.Series(predicted_values_kf.flatten(), index=test_area.index, name='burn_area')\n","\n","        except Exception as e:\n","            print(f\"Failed to model real_id {real_id}: {e}. Falling back to zero prediction.\")\n","            # Fall back to simple zero prediction\n","            predicted_data_df = pd.Series(np.zeros(len(y_test_area)), index=y_test_area.index)\n","\n","    # Ensure alignment\n","    predicted_data_df = predicted_data_df.reindex(y_test_area.index, method='nearest')\n","    predicted_data_df = pd.DataFrame(\n","            {\n","                'ID': str(area) + '_' + test_area.index.strftime('%Y-%m-%d').map(str),\n","                'burn_area': predicted_data_kf.values\n","            }\n","        )\n","    predictions.append(predicted_data_df)\n","\n","    # Calculate RMSE for the current area ID\n","    rmse_area = np.sqrt(mean_squared_error(y_test_area['burn_area'], predicted_data_df))\n","    rmse_values.append(rmse_area)\n","\n","# Calculate the overall RMSE by averaging the RMSE values of all areas\n","if rmse_values:\n","    overall_rmse = np.mean(rmse_values)\n","    print('Overall Test RMSE:', overall_rmse)\n","else:\n","    print('No valid RMSE values were calculated')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_combine = pd.DataFrame(columns=['ID', 'burn_area'])\n","for n in range(48):\n","    df = pd.DataFrame(columns=['ID', 'burn_area'])\n","    for i in range(len(predictions)):\n","        df = pd.concat([df, predictions[i].loc[predictions[n].index == n]], ignore_index=True)\n","    df_combine = pd.concat([df_combine, df], ignore_index=True)\n","sub['burn_area'] = df_combine['burn_area']\n","sub['burn_area'] = sub['burn_area'].clip(0, 1)\n","sub.to_csv('../submissions/submit_02.csv', index=False)"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":14961,"status":"ok","timestamp":1723805497548,"user":{"displayName":"Leonard Mwangi","userId":"02363524813695713194"},"user_tz":-180},"id":"qqKayApZAbrV","outputId":"cab0ab14-3e25-42df-9387-e020fc5b8f8f"},"outputs":[],"source":["df_combine = pd.DataFrame(columns=['ID', 'burn_area'])\n","for n in range(48):\n","    df = pd.DataFrame(columns=['ID', 'burn_area'])\n","    for i in range(len(preds)):\n","        df = pd.concat([df, preds[i].loc[preds[n].index == n]], ignore_index=True)\n","    df_combine = pd.concat([df_combine, df], ignore_index=True)\n","sub['burn_area'] = df_combine['burn_area']\n","sub['burn_area'] = sub['burn_area'].clip(0, 1)\n","sub.to_csv('../submissions/submit_02.csv', index=False)"]}],"metadata":{"colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
